# ML_theory_to_practice

# 📘 Machine Learning: From Theory to Practice  

This repository bridges **machine learning theory** with **practical implementations** and **real-world projects**.  
It follows a structured learning path:  

**Formula → Derivation → Gradient Descent Updates → Implementation → Project**  

---

## 🔹 Repository Structure  

---

## 📂 Sections Overview  

### 1. `theory/` – Mathematical Foundations  
Step-by-step derivations of key ML algorithms:  
- 📄 [Linear Regression](theory/linear_regression_math.md) – MSE cost function, gradient updates  
- 📄 [Logistic Regression](theory/logistic_regression_math.md) – Cross-entropy loss, decision boundary intuition  
- 📄 [Neural Networks](theory/neural_networks_math.md) – Forward/backpropagation derivations  

---

### 2. `implementation/` – From-Scratch Algorithms  
Numpy-based implementations that bring the math to life:  
- 📓 [Linear Regression Implementation](implementation/linear_regression_numpy.ipynb)  
- 📓 [Logistic Regression Implementation](implementation/logistic_regression_numpy.ipynb)  
- 📓 [Neural Networks Implementation](implementation/neural_networks_numpy.ipynb)  

Each notebook includes:  
✔️ Vectorized vs loop-based versions  
✔️ Training on synthetic datasets  
✔️ Comparison with scikit-learn/TensorFlow equivalents  

---

### 3. `projects/` – Real-World Applications  
Applying algorithms to real datasets:  
- 🏠 [House Prices Prediction](projects/house_prices_prediction/)  
- 📧 [Spam Email Classification](projects/spam_email_classification/)  
- ✍️ [Handwritten Digit Recognition](projects/handwritten_digits/)  

Each project folder contains:  
- `data/` (sample or link to dataset)  
- `eda.ipynb` (exploratory data analysis)  
- `model.ipynb` (training & evaluation)  

---

### 4. `docs/` – Extra Resources  
- 📘 [ML Cheat Sheet](docs/ml_cheatsheet.pdf)  
- 🖼️ [Gradient Descent Visuals](docs/gradient_descent_visuals.png)  
- 🔗 [References & Notes](docs/references.md)  

---
