{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ef8751-63a5-4060-9a7b-a4c0ae3c7cdc",
   "metadata": {},
   "source": [
    "# Logistic Regression: Theory and Math\n",
    "\n",
    "This notebook consolidates the mathematical foundation of Logistic Regression, inspired by the *Machine Learning Specialization* by Andrew Ng.  \n",
    "It covers the intuition, formulas, derivations, gradient descent updates, and links to the corresponding NumPy implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a26f5b-978c-47bc-a204-b1a7af1e8c3d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Introduction & Intuition\n",
    "\n",
    "- Logistic Regression is a supervised learning algorithm for **classification** problems.  \n",
    "- Instead of predicting a continuous value (like Linear Regression), it predicts a **probability** that an input belongs to a class.  \n",
    "- The output is always between 0 and 1 using the **sigmoid (logistic) function** for binary classification, or the **softmax function** for multiclass classification.  \n",
    "\n",
    "**Types of Logistic Regression:**\n",
    "\n",
    "| Type | Features | Classes | Activation Function | Notes |\n",
    "|------|----------|--------|-------------------|-------|\n",
    "| **Binary Logistic Regression** | 1 or more | 2 | Sigmoid | Predicts probability of one class; output interpreted as $(P(y=1)$ |\n",
    "| **Multiclass (Multinomial) Logistic Regression** | 1 or more | K ‚â• 3 | Softmax | Predicts probability distribution over K classes; outputs sum to 1 |\n",
    "\n",
    "**Examples (Binary):**\n",
    "- Predicting if a tumor is malignant (1) or benign (0)  \n",
    "- Predicting if an email is spam (1) or not spam (0)\n",
    "\n",
    "**Examples (Multiclass):**\n",
    "- Predicting handwritten digits (0‚Äì9)  \n",
    "- Classifying types of flowers (e.g., iris species)\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression models the probability of class membership by fitting the **sigmoid curve** for binary outcomes or the **softmax function** for multiclass outcomes. The goal is to predict probabilities that align with actual labels while minimizing the **cross-entropy (log-loss) cost**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6942bf-7860-40ca-a5b4-6b4f77116e0c",
   "metadata": {},
   "source": [
    "![Binary Logistic Regression Plot](images/logistic_regression/binary_logistic_regression_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1722d146-ea9d-460e-ae00-4b7ae5508e39",
   "metadata": {},
   "source": [
    "![Multiclass Logistic Regression Plot](images/logistic_regression/multiclass_logistic_regression_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5221f74-3973-4743-8e8c-4edffdfa6f01",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Notations\n",
    "\n",
    "A reference table for symbols used in **binary and multiclass logistic regression** derivations, grouped by concept.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Inputs & Outputs\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $x^{(i)}$ | Scalar | Feature value of sample $i$ | Single feature input for example $i$ |\n",
    "| $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ | Vector | Feature vector of sample $i$ | $[x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]^T$ |\n",
    "| $X \\in \\mathbb{R}^{m \\times n}$ | Matrix | Feature matrix for all $m$ samples | Rows = samples, cols = features |\n",
    "| $y^{(i)}$ | Scalar | Target label of sample $i$ | $y^{(i)} \\in \\{0,1\\}$ (binary) or $\\{1,2,...,K\\}$ (multiclass) |\n",
    "| $Y \\in \\mathbb{R}^{m \\times K}$ | Matrix | One-hot encoded label matrix | Row $i$ corresponds to sample $i$ |\n",
    "\n",
    "\n",
    "### üü° Linear Combination (Logits)\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $z^{(i)}$ | Scalar | Logit (binary) | $z^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$ |\n",
    "| $z_c$ | Scalar | Logit (score) for class $c$ | $z_c = \\mathbf{w}_c^\\top \\mathbf{x}^{(i)} + b_c$ |\n",
    "| $\\mathbf{z}^{(i)} \\in \\mathbb{R}^K$ | Vector | Logits for all classes (sample $i$) | $\\mathbf{z}^{(i)} = W^\\top \\mathbf{x}^{(i)} + \\mathbf{b}$ |\n",
    "\n",
    "### üîµ Predictions\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $\\sigma(z)$ | Function | Sigmoid (binary) | $\\sigma(z) = \\frac{1}{1+e^{-z}}$ |\n",
    "| $\\hat{y}^{(i)}$ | Scalar | Predicted probability for class 1 (binary) | $\\hat{y}^{(i)} = \\sigma(z^{(i)})$ |\n",
    "| $\\hat{y}_c^{(i)}$ | Scalar | Probability that $\\mathbf{x}^{(i)}$ belongs to class $c$ | $\\hat{y}_c^{(i)} = \\frac{e^{z_c^{(i)}}}{\\sum_{j=1}^K e^{z_j^{(i)}}}$ |\n",
    "| $\\hat{\\mathbf{y}}^{(i)} \\in \\mathbb{R}^K$ | Vector | Predicted probability distribution over $K$ classes | $\\hat{\\mathbf{y}}^{(i)} = \\text{softmax}(\\mathbf{z}^{(i)})$ |\n",
    "| $\\hat{Y} \\in \\mathbb{R}^{m \\times K}$ | Matrix | Predicted probability matrix for $m$ samples | Each row = $\\hat{\\mathbf{y}}^{(i)}$ |\n",
    "\n",
    "### üî¥ Parameters\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $w$ | Scalar | Weight / coefficient (binary) | Single-feature logistic regression |\n",
    "| $\\mathbf{w} \\in \\mathbb{R}^n$ | Vector | Weight vector (binary) | $[w_1, w_2, ..., w_n]^T$ |\n",
    "| $W \\in \\mathbb{R}^{n \\times K}$ | Matrix | Weight matrix (multiclass) | Each column = weights for class $c$ |\n",
    "| $b$ | Scalar | Bias term (binary) | Scalar offset |\n",
    "| $\\mathbf{b} \\in \\mathbb{R}^K$ | Vector | Bias terms for $K$ classes | $[b_1, b_2, ..., b_K]^T$ |\n",
    "\n",
    "### üü£ Cost Functions\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $J(w,b)$ | Scalar | Cost function (binary cross-entropy) | $-\\frac{1}{m}\\sum_{i=1}^m \\big[y^{(i)}\\log\\hat{y}^{(i)} + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\big]$ |\n",
    "| $J(W,\\mathbf{b})$ | Scalar | Cost function (multiclass cross-entropy) | $-\\frac{1}{m}\\sum_{i=1}^m \\sum_{c=1}^K y_c^{(i)} \\log \\hat{y}_c^{(i)}$ |\n",
    "\n",
    "### üü§ Training Hyperparameters\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $m$ | Scalar | Number of training examples | Dataset size |\n",
    "| $n$ | Scalar | Number of features | Dimension of $\\mathbf{x}^{(i)}$ |\n",
    "| $\\alpha$ | Scalar | Learning rate | Step size in gradient descent |\n",
    "\n",
    "### ‚ö´ Gradients\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $\\frac{\\partial J}{\\partial w_j}$ | Scalar | Gradient w.r.t $j$-th weight (binary) | Used in updates |\n",
    "| $\\frac{\\partial J}{\\partial W} \\in \\mathbb{R}^{n \\times K}$ | Matrix | Gradient of cost w.r.t weight matrix | Used in multiclass updates |\n",
    "| $\\frac{\\partial J}{\\partial b}$ | Scalar | Gradient w.r.t bias (binary) |  |\n",
    "| $\\frac{\\partial J}{\\partial \\mathbf{b}} \\in \\mathbb{R}^K$ | Vector | Gradient of cost w.r.t bias vector | Used in multiclass updates |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31442cd6-2204-4b95-ab07-1d7ec57c4909",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Model Formula\n",
    "\n",
    "### Single Logistic Regression (binary, 1 feature)\n",
    "\n",
    "$$\n",
    "z^{(i)} = w x^{(i)} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724fb36-c023-406e-8a6e-d564a7bfe9f3",
   "metadata": {},
   "source": [
    "### Multivariable Logistic Regression (binary, multiple features)\n",
    "\n",
    "$$\n",
    "z^{(i)} = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} + b = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(z^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8efb49-3c3a-4b5e-811a-b41b1ec9debf",
   "metadata": {},
   "source": [
    "### Multiclass Logistic Regression (Softmax)\n",
    "\n",
    "For multi-class classification with $k$ classes, logistic regression is generalized using the **softmax function**.\n",
    "\n",
    "The softmax function converts raw scores (logits) for each class into **probabilities that sum to 1**:\n",
    "\n",
    "$$\n",
    "z_c = \\mathbf{w}_c^\\top \\mathbf{x} + b_c\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_c^{(i)} = \\frac{e^{z_c}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc42a0-feb3-4217-b903-f520be805e32",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Cost Function (Log Loss)\n",
    "\n",
    "### Binary Logistic Regression\n",
    "\n",
    "$$\n",
    "L(\\hat{y}^{(i)}, y^{(i)}) = - \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(\\hat{y}^{(i)}, y^{(i)}) =\n",
    "\\begin{cases}\n",
    "    - \\log \\hat{y}^{(i)} & \\text{if $y^{(i)}=1$} \\\\\n",
    "    - \\log \\left( 1 - \\hat{y}^{(i)} \\right) & \\text{if $y^{(i)}=0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})\n",
    "$$\n",
    " \n",
    "---\n",
    "\n",
    "### Multiclass Logistic Regression (Softmax)\n",
    "\n",
    "$$\n",
    "\\hat{y}_c^{(i)} = \\frac{e^{z_c}}{\\sum_{j=1}^{k} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(\\hat{\\mathbf{y}}^{(i)}, \\mathbf{y}^{(i)}) = - \\sum_{c=1}^{k} y^{(i)}_c \\, \\log \\hat{y}^{(i)}_c\n",
    "$$\n",
    "  \n",
    "$$\n",
    "J(W, \\mathbf{b}) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{c=1}^k y_c^{(i)} \\log \\hat{y}_c^{(i)} = -\\frac{1}{m} \\sum_{i=1}^m L(\\hat{\\mathbf{y}}^{(i)}, \\mathbf{y}^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59643b74-02db-4b78-a278-a836891e3d56",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Gradient Descent Derivation\n",
    "\n",
    "Minimize cost $J$ using gradient descent\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    " \n",
    "---\n",
    "\n",
    "### 1. Binary Logistic Regression\n",
    "\n",
    "Gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\big( \\hat{y}^{(i)} - y^{(i)} \\big) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m \\big( \\hat{y}^{(i)} - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Multiclass Logistic Regression (Softmax)\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)}_c = \\frac{e^{z_c^{(i)}}}{\\sum_{j=1}^k e^{z_j^{(i)}}}, \n",
    "\\quad z^{(i)} = W^\\top \\mathbf{x}^{(i)} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = \\frac{1}{m} X^\\top (\\hat{Y} - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}} = \\frac{1}{m} 1^\\top (\\hat{Y} - Y)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963efd3c-7b69-4cc7-874d-88b1a351d751",
   "metadata": {},
   "source": [
    "![Gradient Descent Plot](images/logistic_regression/grad_desc_logistic_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb4a828-361b-4f9c-9900-cc4446a71992",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Vectorized Form\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è. Binary Logistic Regression (Multiple Features)\n",
    "\n",
    "#### Predictions\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\sigma(X \\mathbf{w} + b)\n",
    "$$\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = -\\frac{1}{m} \\Big[ \\mathbf{y}^T \\log(\\hat{\\mathbf{y}}) + (1-\\mathbf{y})^T \\log(1-\\hat{\\mathbf{y}}) \\Big]\n",
    "$$\n",
    "\n",
    "#### Gradient Descent Updates\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\frac{\\alpha}{m} X^T (\\hat{\\mathbf{y}} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\frac{\\alpha}{m} \\sum_{i=1}^m \\big(\\hat{y}^{(i)} - y^{(i)}\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è. Multiclass Logistic Regression (Softmax)\n",
    "\n",
    "#### Predictions (Softmax)\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{softmax}(X W + \\mathbf{b})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)}_c = \\frac{e^{z_c^{(i)}}}{\\sum_{j=1}^k e^{z_j^{(i)}}}, \n",
    "\\quad z^{(i)} = W^\\top \\mathbf{x}^{(i)} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "#### Cost Function (Cross-Entropy)\n",
    "\n",
    "$$\n",
    "J(W, \\mathbf{b}) = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{c=1}^k y_c^{(i)} \\log(\\hat{y}_c^{(i)})\n",
    "$$\n",
    "\n",
    "#### Gradient Descent Updates\n",
    "\n",
    "$$\n",
    "W := W - \\frac{\\alpha}{m} X^T (\\hat{Y} - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b} := \\mathbf{b} - \\frac{\\alpha}{m} \\sum_{i=1}^m (\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727a041-f990-40ec-8338-a5691db50cb6",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Additional Concepts\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "1. **Binary Logistic Regression**\n",
    "- Classification is made using a threshold (commonly 0.5):\n",
    "    - If $\\hat{y}^{(i)} \\geq 0.5$, predict class 1.\n",
    "    - If $\\hat{y}^{(i)} < 0.5$, predict class 0. \n",
    "\n",
    "\n",
    "2. **Multiclass Logistic Regression**\n",
    "- Classification is based on the highest predicted probability:\n",
    "    $$\n",
    "    \\hat{y}^{(i)} = \\arg\\max_c \\hat{y}_c^{(i)}\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization\n",
    "\n",
    "To prevent overfitting, add penalty terms:\n",
    "\n",
    "- **L2 (Ridge)**  \n",
    "$$\n",
    "J_{ridge}(\\mathbf{w}, b) = J(\\mathbf{w}, b) + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2\n",
    "$$\n",
    "\n",
    "Gradient w.r.t $w_j$:  \n",
    "$$\n",
    "\\frac{\\partial J_\\text{ridge}}{\\partial w_j} = \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "*The L2 penalty term $\\frac{\\lambda}{m} w_j$ is proportional to the coefficient itself, so larger coefficients shrink faster, smaller ones shrink slower. Coefficients rarely become exactly zero.*\n",
    "\n",
    "- **L1 (Lasso)**  \n",
    "$$\n",
    "J_{lasso}(\\mathbf{w}, b) = J(\\mathbf{w}, b) + \\frac{\\lambda}{2m} \\sum_{j=1}^n |w_j|\n",
    "$$\n",
    "\n",
    "Gradient w.r.t $w_j$:  \n",
    "$$\n",
    "\\frac{\\partial J_\\text{lasso}}{\\partial w_j} = \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} + \\frac{\\lambda}{2m} \\, \\text{sign}(w_j)\n",
    "$$\n",
    "\n",
    "*The L1 penalty term $\\frac{\\lambda}{2m} \\text{sign}(w_j)$ pushes weights toward zero at a constant rate. Some coefficients can become exactly zero, making Lasso useful for feature selection.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29280a14-8555-4ca4-b795-94b58964a45f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Implementation\n",
    "\n",
    "See the corresponding **NumPy-based implementation** here: [logistic_regression_numpy.ipynb](../implementation/logistic_regression_numpy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
