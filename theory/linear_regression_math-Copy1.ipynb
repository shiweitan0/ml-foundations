{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbdf6e4-a37b-49c3-84bf-b4cca3b92c23",
   "metadata": {},
   "source": [
    "# Linear Regression: Theory and Math\n",
    "\n",
    "This notebook consolidates the mathematical foundation of Linear Regression, inspired by the *Machine Learning Specialization* by Andrew Ng.  \n",
    "It covers the intuition, formulas, derivations, gradient descent updates, and links to the corresponding implementation in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4293fb-4693-49c5-8324-471aa2d13f8e",
   "metadata": {},
   "source": [
    "## 1️⃣ Introduction & Intuition\n",
    "\n",
    "Linear Regression is a fundamental supervised learning algorithm used to predict a continuous target variable from one or more input features.\n",
    "\n",
    "- When there is **only one input feature**, it is called **Single Linear Regression**.  \n",
    "  *Example:* Predicting house price based only on square footage.\n",
    "\n",
    "- When there are **multiple input features**, it is called **Multiple Linear Regression**.  \n",
    "  *Example:* Predicting house price based on square footage, number of bedrooms, and location.\n",
    "\n",
    "\n",
    "**More Examples**\n",
    "- Estimating house prices  \n",
    "- Forecasting sales revenue from marketing spend across different channels  \n",
    "- Predicting stock returns from multiple financial indicators\n",
    "\n",
    "---\n",
    "Linear regression fits a line (or a hyperplane in multiple dimensions) that best captures the relationship between features and the target variable. The goal is to minimize the difference between predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05de892-0c89-486a-b2da-7ad473fe5bc0",
   "metadata": {},
   "source": [
    "![Single Linear Regression Plot](images/linear_regression/single_linear_regression_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeaed5d-9a90-4b6e-88a1-df01e0653aa9",
   "metadata": {},
   "source": [
    "## 2️⃣ Notation\n",
    "\n",
    "A reference table for symbols used in **single and multiple linear regression** derivations.\n",
    "\n",
    "| Symbol | Type | Meaning | Example / Notes |\n",
    "|--------|------|---------|----------------|\n",
    "| $x^{(i)}$ | Scalar | Feature value of sample $i$ | Single feature input for example $i$ |\n",
    "| $y^{(i)}$ | Scalar | Target value of sample $i$ | Label for example $i$ |\n",
    "| $w$ | Scalar | Weight / slope | Single linear regression coefficient |\n",
    "| $b$ | Scalar | Bias / intercept | Single linear regression intercept |\n",
    "| $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ | Vector | Feature vector of sample $i$ | $[x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]^T$ |\n",
    "| $\\mathbf{w} \\in \\mathbb{R}^n$ | Vector | Weight vector | $[w_1, w_2, ..., w_n]^T$ |\n",
    "| $\\hat{y}^{(i)}$ | Scalar | Predicted value for sample $i$ | $w x^{(i)} + b$ (single) or $\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$ (multiple) |\n",
    "| $m$ | Scalar | Number of training examples | Dataset size |\n",
    "| $n$ | Scalar | Number of features | Dimension of feature vector $\\mathbf{x}^{(i)}$ |\n",
    "| $J(w,b)$ | Scalar | Cost function (MSE) | $\\frac{1}{2m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2$ (single) |\n",
    "| $J(\\mathbf{w},b)$ | Scalar | Cost function (MSE) | $\\frac{1}{2m}\\sum_{i=1}^m (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b - y^{(i)})^2$ |\n",
    "| $\\alpha$ | Scalar | Learning rate | Step size in gradient descent |\n",
    "| $w_j$ | Scalar | Weight of feature $j$ | Only used in multiple linear regression |\n",
    "| $\\frac{\\partial J}{\\partial w}$ | Scalar | Gradient w.r.t weight | Single linear regression |\n",
    "| $\\frac{\\partial J}{\\partial w_j}$ | Scalar | Gradient w.r.t $j$-th weight | Multiple linear regression |\n",
    "| $\\frac{\\partial J}{\\partial b}$ | Scalar | Gradient w.r.t bias | Same for single and multiple |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd076f4c-2454-4fb4-b40c-0d92c6d82656",
   "metadata": {},
   "source": [
    "## 3️⃣ Model Formula  \n",
    "\n",
    "The linear regression model predicts output as a weighted sum of the input features plus a bias.  \n",
    "\n",
    "1) **Single Linear Regression (one feature):**  \n",
    "  $$\n",
    "  f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "  $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba95a74-2e28-4dfa-b85d-2c1759154e6d",
   "metadata": {},
   "source": [
    "![Single Linear Regression](images/linear_regression/single_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e194e23-4ce8-4c6b-8be7-f00bfc34ddd8",
   "metadata": {},
   "source": [
    "2) **Multiple Linear Regression (multiple features):**  \n",
    "  $$\n",
    "  f_{\\mathbf{w},b}(x^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} + b\n",
    "  $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92597f-330b-4e0e-a9b7-8979334a12b7",
   "metadata": {},
   "source": [
    "![Multiple Linear Regression](images/linear_regression/multiple_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7010c-e037-472f-a598-deba0fb26171",
   "metadata": {},
   "source": [
    "## 4️⃣ Cost Function\n",
    "\n",
    "The cost function measures how far the model predictions are from the actual data. It is used to guide the optimization of the model parameters.\n",
    "\n",
    "1. **Single Linear Regression**\n",
    "\n",
    "For a single feature $x^{(i)}$, the cost function is:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} \\left(f_{w,b}(x^{(i)}) - y^{(i)}\\right)^2, \\quad \\text{where} \\quad \\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "$$\n",
    "\n",
    "- The factor $\\frac{1}{2}$ simplifies derivatives during gradient descent.  \n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the cost function generalizes to:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2, \\quad \\text{where} \\quad \\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} + b\n",
    "$$\n",
    "\n",
    "---\n",
    " Goal: Minimize the cost function by adjusting the weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963efc1-f8dd-48a2-b6da-50bcc6865e35",
   "metadata": {},
   "source": [
    "## 5️⃣ Gradient Descent Derivation\n",
    "\n",
    "Gradient descent is the optimization method used to minimize the cost function $J$. It updates the model parameters iteratively in the direction that reduces the cost.\n",
    "\n",
    "1. **Single Linear Regression**\n",
    "\n",
    "For a single feature $x^{(i)}$, the parameters are updated as:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( (w x^{(i)} + b) - y^{(i)} \\big) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( (w x^{(i)} + b) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the weights vector $\\mathbf{w} = [w_1, w_2, ..., w_n]$ and bias $b$ are updated as:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}, \\quad \\text{for } j = 1,2,...,n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "> Note:\n",
    "> $:=$ means to *update* the value on the **left** with the value on the **right**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f7eb6-15a3-4e12-8b35-a70867061a88",
   "metadata": {},
   "source": [
    ">**Note on Visualization Choice**\n",
    ">\n",
    ">In the following 2 plots, gradient descent is demonstrated without using a bias term (b). The intent behind this simplification is purely to enhance visualization and intuition: by removing b, the cost function becomes a one-dimensional curve in terms of w only. This allows the gradient descent path to lie exactly on the cost curve, making it easier to see how each update moves “downhill” toward the minimum.\n",
    "\n",
    ">It is important to highlight that in a full linear regression model, both w and b would be updated simultaneously. The 2D visualization here does not capture the interplay between slope and intercept; it is a deliberate simplification to illustrate the basic mechanism of gradient descent in a clear and intuitive way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452e82e-3abf-4d0c-b560-e8c439c93234",
   "metadata": {},
   "source": [
    "##### Gradient descent with **good learning rate**\n",
    "![Gradient Descent Good Learning Rate](images/linear_regression/grad_desc_good_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb7fd0-9bc2-4e88-b1ab-9ab70d6e8022",
   "metadata": {},
   "source": [
    "##### Gradient descent with **bad learning rate**\n",
    "![Gradient Descent Bad Learning Rate](images/linear_regression/grad_desc_bad_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57340a-c87b-486e-8870-972499af9dca",
   "metadata": {},
   "source": [
    "## 6️⃣ Vectorized Form (Scalable for Real Datasets)\n",
    "\n",
    "Linear regression can be expressed in **matrix/vector form** for computational efficiency, using a **weight vector $\\mathbf{w}$** and **bias $b$** consistently.\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple Linear Regression (General Case)\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the model predicts:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = X \\mathbf{w} + b\n",
    "$$\n",
    "\n",
    "- $X \\in \\mathbb{R}^{m \\times n}$ contains feature values for all examples:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_1^{(0)} & x_2^{(0)} & \\dots & x_n^{(0)} \\\\\n",
    "x_1^{(1)} & x_2^{(1)} & \\dots & x_n^{(1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^{(m-1)} & x_2^{(m-1)} & \\dots & x_n^{(m-1)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{n \\times 1}$ is the weight vector  \n",
    "- $b \\in \\mathbb{R}$ is the bias scalar (broadcasted across examples)  \n",
    "- $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{m \\times 1}$ is the vector of predictions  \n",
    "\n",
    "**Gradient Descent Updates (Vectorized):**\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\frac{\\alpha}{m} X^T (X \\mathbf{w} + b - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\frac{\\alpha}{m} \\sum_{i=0}^{m-1} \\left( (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "> **Note:** \n",
    "> - $\\alpha$ is the learning rate.  \n",
    "> - $X \\mathbf{w} + b$ computes predictions for all $m$ examples simultaneously.  \n",
    "> - Vectorized gradient computation eliminates explicit loops and is more computationally efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Vectorization is Useful\n",
    "\n",
    "- Eliminates explicit loops over training examples and features.  \n",
    "- Leverages **broadcasting** in libraries like NumPy for efficient computation.  \n",
    "- Particularly beneficial for datasets with many features ($n$ large).  \n",
    "- Simplifies implementation and speeds up gradient descent updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335ae8d-1dd4-4c78-a31c-1027433a875e",
   "metadata": {},
   "source": [
    "## 7️⃣ Additional Concepts\n",
    "\n",
    "### Overfitting & Underfitting\n",
    "\n",
    "- **Underfitting**: Model is too simple; high cost even at optimal parameters.  \n",
    "- **Overfitting**: Model fits training data extremely well (low cost) but generalizes poorly to new data.  \n",
    "\n",
    "---\n",
    "\n",
    "### Regularization (Ridge / Lasso)\n",
    "\n",
    "Adds a penalty to the cost function to prevent overfitting.\n",
    "\n",
    "1. **Ridge (L2)** — shrinks coefficients proportionally:\n",
    "\n",
    "$$\n",
    "J_\\text{ridge}(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b - y^{(i)} \\big)^2 \n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "Gradient w.r.t $w_j$:  \n",
    "$$\n",
    "\\frac{\\partial J_\\text{ridge}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} w_j\n",
    "$$\n",
    "\n",
    "> Large coefficients shrink faster, small ones slower; rarely exactly zero.\n",
    "\n",
    "2. **Lasso (L1)** — encourages sparsity (feature selection):\n",
    "\n",
    "$$\n",
    "J_\\text{lasso}(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b - y^{(i)} \\big)^2 \n",
    "+ \\frac{\\lambda}{2m} \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "Gradient w.r.t $w_j$:  \n",
    "$$\n",
    "\\frac{\\partial J_\\text{lasso}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{2m} \\, \\text{sign}(w_j)\n",
    "$$\n",
    "\n",
    "> L1 penalty can push coefficients exactly to zero, enabling feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4018817-b667-4579-b590-2fb023405d73",
   "metadata": {},
   "source": [
    "## 8️⃣ Implementation\n",
    "\n",
    "See the corresponding **NumPy-based implementation** here:  [linear_regression_numpy.ipynb](../implementation/linear_regression_numpy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
