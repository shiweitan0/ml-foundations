{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254d5a9e-d4db-4cf5-bd25-309480420401",
   "metadata": {},
   "source": [
    "# Linear Regression: Theory and Math\n",
    "\n",
    "This notebook consolidates the mathematical foundation of Linear Regression, inspired by the *Machine Learning Specialization* by Andrew Ng.  \n",
    "It covers the intuition, formulas, derivations, gradient descent updates, and links to the corresponding implementation in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4293fb-4693-49c5-8324-471aa2d13f8e",
   "metadata": {},
   "source": [
    "## 1️⃣ Introduction & Intuition\n",
    "\n",
    "Linear Regression is a fundamental supervised learning algorithm used to predict a continuous target variable from one or more input features.\n",
    "\n",
    "- When there is **only one input feature**, it is called **Single Linear Regression**.  \n",
    "  *Example:* Predicting house price based only on square footage.\n",
    "\n",
    "- When there are **multiple input features**, it is called **Multiple Linear Regression**.  \n",
    "  *Example:* Predicting house price based on square footage, number of bedrooms, and location.\n",
    "\n",
    "\n",
    "**More Examples**\n",
    "- Estimating house prices  \n",
    "- Forecasting sales revenue from marketing spend across different channels  \n",
    "- Predicting stock returns from multiple financial indicators\n",
    "\n",
    "---\n",
    "Linear regression fits a line (or a hyperplane in multiple dimensions) that best captures the relationship between features and the target variable. The goal is to minimize the difference between predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05de892-0c89-486a-b2da-7ad473fe5bc0",
   "metadata": {},
   "source": [
    "![Single Linear Regression Plot](images/single_linear_regression_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd076f4c-2454-4fb4-b40c-0d92c6d82656",
   "metadata": {},
   "source": [
    "## 2️⃣ Model Formula  \n",
    "\n",
    "The linear regression model predicts output as a weighted sum of the input features plus a bias.  \n",
    "\n",
    "1) **Single Linear Regression (one feature):**  \n",
    "  $$\n",
    "  f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "  $$  \n",
    "  • $w$: slope (weight)  \n",
    "  • $b$: intercept (bias)  \n",
    "  • $x^{(i)}$: input feature for sample $i$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba95a74-2e28-4dfa-b85d-2c1759154e6d",
   "metadata": {},
   "source": [
    "![Single Linear Regression](images/single_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e194e23-4ce8-4c6b-8be7-f00bfc34ddd8",
   "metadata": {},
   "source": [
    "2) **Multiple Linear Regression (multiple features):**  \n",
    "  $$\n",
    "  f_{\\mathbf{w},b}(x^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} + b\n",
    "  $$  \n",
    "  or, in vector form:  \n",
    "  $$\n",
    "  f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b\n",
    "  $$  \n",
    "  • $\\mathbf{w} = (w_1, w_2, \\dots, w_n)$: weight vector (slopes) <br>\n",
    "• $n$: number of features <br>\n",
    "  • $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\dots, x_n^{(i)})$: feature vector for sample $i$  \n",
    "  • $b$: intercept (bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92597f-330b-4e0e-a9b7-8979334a12b7",
   "metadata": {},
   "source": [
    "![Multiple Linear Regression](images/multiple_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7010c-e037-472f-a598-deba0fb26171",
   "metadata": {},
   "source": [
    "## 3️⃣ Cost Function\n",
    "\n",
    "The cost function measures how far the model predictions are from the actual data. It is used to guide the optimization of the model parameters.\n",
    "\n",
    "1. **Single Linear Regression**\n",
    "\n",
    "For a single feature $x^{(i)}$, the cost function is:\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} \\left(f_{w,b}(x^{(i)}) - y^{(i)}\\right)^2, \\quad \\text{where} \\quad \\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b\n",
    "$$\n",
    "\n",
    "- $m$ is the number of training examples.  \n",
    "- The factor $\\frac{1}{2}$ simplifies derivatives during gradient descent.  \n",
    "- Goal: Minimize $J(w,b)$ by adjusting $w$ (slope) and $b$ (bias).  \n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the cost function generalizes to:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w},b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} \\left(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2, \\quad \\text{where} \\quad \\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = w_1 x_1^{(i)} + w_2 x_2^{(i)} + \\dots + w_n x_n^{(i)} + b\n",
    "$$\n",
    "\n",
    "- $n$ is the number of features.\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_n]$ are the weights for each feature.  \n",
    "- Goal: Minimize $J(\\mathbf{w},b)$** by adjusting weights $w_j$ and bias $b$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963efc1-f8dd-48a2-b6da-50bcc6865e35",
   "metadata": {},
   "source": [
    "## 4️⃣ Gradient Descent Derivation\n",
    "\n",
    "Gradient descent is the optimization method used to minimize the cost function $J$. It updates the model parameters iteratively in the direction that reduces the cost.\n",
    "\n",
    "1. **Single Linear Regression**\n",
    "\n",
    "For a single feature $x^{(i)}$, the parameters are updated as:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}, \\quad\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( (w x^{(i)} + b) - y^{(i)} \\big) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( (w x^{(i)} + b) - y^{(i)} \\big)\n",
    "$$\n",
    "\n",
    "- $\\alpha$ is the **learning rate** (step size).  \n",
    "- Goal: Iteratively move all parameters toward the minimum of $J(w,b)$.\n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the weights vector $\\mathbf{w} = [w_1, w_2, ..., w_n]$ and bias $b$ are updated as:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}, \\quad \\text{for } j = 1,2,...,n\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "- Each weight $w_j$ is adjusted according to its partial derivative of the cost.  \n",
    "- The bias $b$ is updated similarly.  \n",
    "- Goal: Iteratively move all parameters toward the minimum of $J(\\mathbf{w},b)$.\n",
    "\n",
    "> Note:\n",
    "> $:=$ means to *update* the value on the **left** with the value on the **right**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f7eb6-15a3-4e12-8b35-a70867061a88",
   "metadata": {},
   "source": [
    ">**Note on Visualization Choice**\n",
    ">\n",
    ">In the following 2 plots, gradient descent is demonstrated without using a bias term (b). The intent behind this simplification is purely to enhance visualization and intuition: by removing b, the cost function becomes a one-dimensional curve in terms of w only. This allows the gradient descent path to lie exactly on the cost curve, making it easier to see how each update moves “downhill” toward the minimum.\n",
    "\n",
    ">It is important to highlight that in a full linear regression model, both w and b would be updated simultaneously. The 2D visualization here does not capture the interplay between slope and intercept; it is a deliberate simplification to illustrate the basic mechanism of gradient descent in a clear and intuitive way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452e82e-3abf-4d0c-b560-e8c439c93234",
   "metadata": {},
   "source": [
    "##### Gradient descent with **good learning rate**\n",
    "![Gradient Descent Good Learning Rate](images/grad_desc_good_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb7fd0-9bc2-4e88-b1ab-9ab70d6e8022",
   "metadata": {},
   "source": [
    "##### Gradient descent with **bad learning rate**\n",
    "![Gradient Descent Bad Learning Rate](images/grad_desc_bad_lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c704b2-fb23-4612-8e20-3323076056b7",
   "metadata": {},
   "source": [
    "## 5️⃣ Vectorized Form\n",
    "\n",
    "Linear regression can be expressed in **matrix/vector form** for computational efficiency.\n",
    "\n",
    "1. **Single Linear Regression**\n",
    "\n",
    "For a single feature, the model can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = X \\theta\n",
    "$$\n",
    " \n",
    "- $X$ is an $m \\times 2$ matrix, with the first column containing the feature $x^{(i)}$, and the second column contains 1's for the bias\n",
    " \n",
    "\\begin{bmatrix}\n",
    "x^{(0)} & 1 \\\\\n",
    "x^{(1)} & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x^{(m-1)} & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "- $\\theta = \\begin{bmatrix} w \\\\ b \\end{bmatrix}$  \n",
    "- $\\mathbf{\\hat{y}}$ is the vector of predictions  \n",
    "- Gradient descent update:  \n",
    "$$\n",
    "\\theta := \\theta - \\frac{\\alpha}{m} X^T (X \\theta - y)\n",
    "$$\n",
    "\n",
    "2. **Multiple Linear Regression**\n",
    "\n",
    "For multiple features $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}]$, the vectorized form generalizes naturally:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = X \\mathbf{\\theta}, \\quad \n",
    "\\mathbf{\\theta} := \\mathbf{\\theta} - \\frac{\\alpha}{m} X^T (X \\mathbf{\\theta} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "- $X$ is an $m \\times (n+1)$ matrix, with the first $n$ columns containing features $x_1^{(i)}, ..., x_n^{(i)}$, and the last column is 1's for the bias\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_1^{(0)} & x_2^{(0)} & \\dots & x_n^{(0)} & 1 \\\\\n",
    "x_1^{(1)} & x_2^{(1)} & \\dots & x_n^{(1)} & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x_1^{(m-1)} & x_2^{(m-1)} & \\dots & x_n^{(m-1)} & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "- $\\mathbf{\\theta} = \\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_n \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "$  \n",
    "- $\\mathbf{\\hat{y}}$ is the $m \\times 1$ vector of predictions\n",
    "\n",
    "---\n",
    "\n",
    "Why vectorization is useful\n",
    "  - Eliminates explicit loops over training examples and features.  \n",
    "  - Leverages **broadcasting** in libraries like NumPy, automatically applying operations (like adding a bias) across all elements or rows.  \n",
    "  - Particularly beneficial for **multiple linear regression**, where the number of features $n$ is large.  \n",
    "  - Reduces computational overhead and speeds up gradient descent updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14738b6-5e10-47b5-b83b-215ded351a03",
   "metadata": {},
   "source": [
    "## 6️⃣ Additional Concepts\n",
    "\n",
    "**Overfitting & Underfitting**\n",
    "\n",
    "Using too many features relative to the number of data points can lead to overfitting, while using too few features may lead to underfitting. \n",
    "\n",
    "  - **Underfitting**: the model is too simple, and the cost function remains high even at the optimal parameters.  \n",
    "  - **Overfitting**: the model fits the training data very well, so the cost is extremely low for the training set, but it may generalize poorly.  \n",
    "\n",
    "---\n",
    "\n",
    "**Regularization (Ridge / Lasso)** \n",
    "\n",
    "- Adds a penalty term to the cost function to reduce overfitting\n",
    "  \n",
    "1. **Ridge (L2)** — coefficient shrinkage\n",
    "   \n",
    "    $$\n",
    "    J_{ridge}(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\sum_{j=1}^{n} w_j x_j^{(i)} + b - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "    $$\n",
    "\n",
    "Gradient w.r.t $w_j$  \n",
    "    $$\n",
    "    \\frac{\\partial J_\\text{ridge}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + 2\\lambda w_j\n",
    "    $$\n",
    "\n",
    "*The L2 penalty term $2\\lambda w_j$ is proportional to the coefficient itself, so large coefficients shrink faster, small ones shrink slower. Coefficients rarely become exactly zero.*\n",
    "\n",
    "2. **Lasso (L1)** — feature selection\n",
    " \n",
    "    $$\n",
    "    J_{lasso}(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(\\sum_{j=1}^{n} w_j x_j^{(i)} + b - y^{(i)}\\Big)^2 + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "    $$\n",
    "\n",
    "Gradient w.r.t $w_j$\n",
    "    $$\n",
    "\\frac{\\partial J_\\text{lasso}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x_j^{(i)} + \\lambda \\, \\text{sign}(w_j)\n",
    "    $$\n",
    "\n",
    "*The L1 penalty term $\\lambda\\,\\text{sign}(w_j)$ is constant in magnitude regardless of $w_j$, which can push small coefficients exactly to zero, enabling feature selection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4018817-b667-4579-b590-2fb023405d73",
   "metadata": {},
   "source": [
    "## 7️⃣ Implementation\n",
    "\n",
    "See the corresponding **NumPy-based implementation** here:  [linear_regression_numpy.ipynb](../implementation/linear_regression_numpy.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
